{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5b8cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94133f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "7ef491d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates a polynomial sum_i(a[i] * x^i)\n",
    "def eval_polynomial(x, *a):\n",
    "        y=0\n",
    "        for i, ai in enumerate(a):\n",
    "            y += ai * pow(x, i)\n",
    "        return y\n",
    "\n",
    "# creates a polynomial with fixed parameters\n",
    "def create_polynomial(*a):\n",
    "    print(f'Creating polynomial with params {a}')\n",
    "    return lambda x : eval_polynomial(x, *a)\n",
    "\n",
    "# y = a\n",
    "def create_constant(a):\n",
    "    return create_polynomial(a)\n",
    "\n",
    "# y = ax + b\n",
    "def create_linear(a, b):\n",
    "    return create_polynomial(b, a)\n",
    "\n",
    "# y = ax^2 + bx + c\n",
    "def create_quadratic(a, b, c):\n",
    "    return create_polynomial(c, b, a)\n",
    "\n",
    "# y = ax^3 + bx^2 + c*x + d\n",
    "def create_cubic(a, b, c, d):\n",
    "    return create_polynomial(d, c, b, a)\n",
    "\n",
    "# y = ax^4 + bx^3 + c*x^2 + d*x + e\n",
    "def create_quartic(a, b, c, d, e):\n",
    "    return create_polynomial(e, d, c, b, a)\n",
    "\n",
    "def random_x(n, min=-10, max=10):\n",
    "    return [random.uniform(min, max) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "d41fde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quadratic(nn.Module):\n",
    "    \"\"\" Custom layer that maps [x] -> [x^2, x]\"\"\"\n",
    "    def __init__(self, scale):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Divide by some linear factor to improve stability (prevents Inf overflow).\n",
    "        return torch.cat((torch.div(torch.square(x), self.scale), x), 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "afc4a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    \"\"\" Linear NN with one input, one hidden layer, and one output.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "#             nn.Linear(1, 1),\n",
    "            nn.Linear(1, n),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n, 1),       \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        yh = self.layer_stack(x)\n",
    "        return yh\n",
    "    \n",
    "class SimpleQuadraticModel(nn.Module):\n",
    "    \"\"\" Quadratic (parabolic) NN with one input, one hidden layer, and one output.\"\"\"\n",
    "    def __init__(self, n, scale):\n",
    "        super(SimpleQuadraticModel, self).__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "#             Quadratic(),    # [x] -> [x, x^2]\n",
    "#             nn.Linear(2, 1),   # hi = [w0*x + w1*x^2] + wb\n",
    "            Quadratic(scale),    # [x] -> [x, x^2]\n",
    "            nn.Linear(2, n),   # hi = [w0*x + w1*x^2] + wb\n",
    "            Quadratic(scale),    # [h] -> [h] + [h^2]\n",
    "            nn.Linear(2*n, 1), # y = [w0*h + w1*h^2] + wb \n",
    "        )\n",
    "#         with torch.no_grad():\n",
    "#             self.layer_stack[1].weight[0][0] = 0.\n",
    "#             self.layer_stack[3].weight[0][0] = 0.\n",
    "\n",
    "    def forward(self, x):\n",
    "        yh = self.layer_stack(x)\n",
    "        return yh\n",
    "    \n",
    "class SimpleFunctionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, fn):\n",
    "        super(SimpleFunctionDataset, self).__init__()\n",
    "        # stick each item in its own list so we have n items of size 1 instead of 1 item of size n.\n",
    "        self.y = torch.tensor(list(map(lambda yi: [fn(yi)], x)), dtype=torch.float).to(device)\n",
    "        self.x = torch.tensor(list(map(lambda xi: [xi], x)), dtype=torch.float).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"x\": self.x[idx], \"y\": self.y[idx]}\n",
    "        return sample\n",
    "    \n",
    "def paramsAsList(model):\n",
    "    params = []\n",
    "    for p in model.parameters():\n",
    "        params.append(p.tolist())\n",
    "    return params\n",
    "\n",
    "class LQTrainer:\n",
    "    def __init__(self, model_l, model_q, lr=0.01):\n",
    "        self.nnl = model_l\n",
    "        self.nnq = model_q\n",
    "        self.optimizer_l = torch.optim.SGD(self.nnl.parameters(), lr = lr) \n",
    "        self.optimizer_q = torch.optim.SGD(self.nnq.parameters(), lr = lr) \n",
    "        self.all_losses_l = [] \n",
    "        self.all_losses_q = [] \n",
    "        self.plot_every = 50 \n",
    "        print(f'lr = {lr}')\n",
    "\n",
    "    def clearLossHistory(self):\n",
    "        self.all_losses_l = [] \n",
    "        self.all_losses_q = [] \n",
    "    \n",
    "    def print_params(self):\n",
    "        print(f'L: {paramsAsList(self.nnl)}')\n",
    "        print(f'Q: {paramsAsList(self.nnq)}')\n",
    "    \n",
    "    def plot_error(self):\n",
    "        plt.plot(trainer.all_losses_l, color=\"Blue\", label=\"Linear\")\n",
    "        plt.plot(trainer.all_losses_q, color=\"Orange\", label=\"Quadratic\")\n",
    "        plt.ylabel('Log Loss')\n",
    "        plt.legend()        \n",
    "    \n",
    "    def train(self, epochs, train_data):\n",
    "        current_loss_l = 0 \n",
    "        current_loss_q = 0 \n",
    "\n",
    "        for epoch in range(epochs): \n",
    "            # remove current gradients for next iteration   \n",
    "            # input training example and return the prediction   \n",
    "            yl = self.nnl.forward(train_data.x)\n",
    "            yq = self.nnq.forward(train_data.x)\n",
    "\n",
    "            # calculate MSE loss   \n",
    "            loss_l = mseloss(yl, train_data.y)\n",
    "            loss_q = mseloss(yq, train_data.y)\n",
    "#             print(f'yq={yq.tolist()} y={train_data.y.tolist()} mse={loss_q.tolist()}')\n",
    "\n",
    "            if loss_l.isnan():\n",
    "                print(f'Linear Loss is NaN. Halting at epoch {epoch}.')\n",
    "                break\n",
    "\n",
    "            if loss_l.isinf():\n",
    "                print(f'Linear Loss is Inf. Halting at epoch {epoch}.')\n",
    "                break\n",
    "\n",
    "            if loss_q.isnan():\n",
    "                print(f'Quadratic Loss is NaN. Halting at epoch {epoch}.')\n",
    "                break\n",
    "\n",
    "            if loss_q.isinf():\n",
    "                print(f'Quadratic Loss is Inf. Halting at epoch {epoch}.')\n",
    "                break\n",
    "\n",
    "            # backpropogate through the loss gradiants   \n",
    "            self.optimizer_l.zero_grad() \n",
    "            self.optimizer_q.zero_grad() \n",
    "\n",
    "            loss_l.backward()\n",
    "            loss_q.backward()\n",
    "\n",
    "            # update model weights   \n",
    "            self.optimizer_l.step()\n",
    "            self.optimizer_q.step()\n",
    "\n",
    "            # append to loss   \n",
    "            current_loss_l += loss_l  \n",
    "            current_loss_q += loss_q  \n",
    "\n",
    "            if epoch % self.plot_every == 0:\n",
    "                self.all_losses_l.append(math.log(0.00000000001+(current_loss_l / self.plot_every).item()))       \n",
    "                self.all_losses_q.append(math.log(0.00000000001+(current_loss_q / self.plot_every).item()))       \n",
    "                print(f'Epoch: {epoch} L Loss: {(current_loss_l / self.plot_every)} Q Loss: {(current_loss_q / self.plot_every)}')\n",
    "                current_loss_l = 0 \n",
    "                current_loss_q = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "133c9512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating polynomial with params (-4.860625131081104,)\n"
     ]
    }
   ],
   "source": [
    "fn = create_constant(random.uniform(-10,10))\n",
    "qscale = 32\n",
    "lr = 1e-2\n",
    "\n",
    "# fn = create_linear(random.uniform(-10,10), random.uniform(-10,10))\n",
    "# qscale = 32\n",
    "# lr = 1e-2\n",
    "\n",
    "# fn = create_quadratic(random.uniform(-10,10), random.uniform(-10,10), random.uniform(-10,10))\n",
    "# qscale = 8\n",
    "# lr = 1e-3\n",
    "\n",
    "# fn = create_cubic(random.uniform(-10,10), random.uniform(-10,10), random.uniform(-10,10), random.uniform(-10,10))\n",
    "# qscale = 32\n",
    "# lr = 1e-3\n",
    "\n",
    "# fn = create_quartic(random.uniform(-1,1), random.uniform(-1,1), random.uniform(-1,1), random.uniform(-1,1), random.uniform(-1,1))\n",
    "# qscale = 4\n",
    "# lr = 1e-4\n",
    "\n",
    "\n",
    "train_data = SimpleFunctionDataset(x=random_x(10000, -5, 5), fn=fn) # 1000 training examples\n",
    "test_data = SimpleFunctionDataset(x=random_x(100), fn=fn)   #  100 testing examples\n",
    "# do we need separate training and testing data for simple generated data like this?\n",
    "# probably... we need to make sure the quadratics don't overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "0d193d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.01\n"
     ]
    }
   ],
   "source": [
    "nnl = SimpleLinearModel(1).to(device)\n",
    "nnq = SimpleQuadraticModel(1, qscale).to(device)\n",
    "nnl = nnl.to(device)\n",
    "nnq = nnq.to(device)\n",
    "\n",
    "trainer = LQTrainer(nnl, nnq, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "062fd54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: [[[0.7473815679550171]], [0.42331576347351074], [[0.709331750869751]], [-0.2039574384689331]]\n",
      "Q: [[[-0.5841401219367981, -0.312803715467453]], [0.590824544429779], [[-0.13348376750946045, -0.14074617624282837]], [-0.07717013359069824]]\n",
      "Epoch: 0 L Loss: 0.6197078824043274 Q Loss: 0.445467472076416\n",
      "Epoch: 50 L Loss: 8.143692970275879 Q Loss: 5.477356433868408\n",
      "Epoch: 100 L Loss: 1.4278010129928589 Q Loss: 0.0070684500969946384\n",
      "Epoch: 150 L Loss: 0.2816992402076721 Q Loss: 0.005136716179549694\n",
      "Epoch: 200 L Loss: 0.002009584568440914 Q Loss: 0.0038179876282811165\n",
      "Epoch: 250 L Loss: 7.675896085856948e-06 Q Loss: 0.0028454812709242105\n",
      "Epoch: 300 L Loss: 2.7955913139976474e-08 Q Loss: 0.0021253868471831083\n",
      "Epoch: 350 L Loss: 1.0352120549672605e-10 Q Loss: 0.0015904296888038516\n",
      "Epoch: 400 L Loss: 2.2803850821540017e-12 Q Loss: 0.0011919235112145543\n",
      "Epoch: 450 L Loss: 2.048063107995546e-12 Q Loss: 0.0008944006985984743\n",
      "Epoch: 500 L Loss: 2.0480188725469084e-12 Q Loss: 0.0006718592485412955\n",
      "Epoch: 550 L Loss: 2.048085442560299e-12 Q Loss: 0.0005051398184150457\n",
      "Epoch: 600 L Loss: 2.048018655706474e-12 Q Loss: 0.0003800790582317859\n",
      "Epoch: 650 L Loss: 2.0480821899537816e-12 Q Loss: 0.0002861633838620037\n",
      "Epoch: 700 L Loss: 2.048018655706474e-12 Q Loss: 0.00021557176660280675\n",
      "Epoch: 750 L Loss: 2.0480852257198645e-12 Q Loss: 0.00016247104213107377\n",
      "Epoch: 800 L Loss: 2.0480438091968756e-12 Q Loss: 0.00012249602878000587\n",
      "Epoch: 850 L Loss: 2.048053567016428e-12 Q Loss: 9.239170321961865e-05\n",
      "Epoch: 900 L Loss: 2.0480277630047228e-12 Q Loss: 6.970281538087875e-05\n",
      "Epoch: 950 L Loss: 2.0480342682177577e-12 Q Loss: 5.260212128632702e-05\n",
      "Epoch: 1000 L Loss: 2.048091947773334e-12 Q Loss: 3.970486068283208e-05\n",
      "Epoch: 1050 L Loss: 2.0480188725469084e-12 Q Loss: 2.9973904020152986e-05\n",
      "Epoch: 1100 L Loss: 2.048085442560299e-12 Q Loss: 2.2632213585893624e-05\n",
      "Epoch: 1150 L Loss: 2.048018655706474e-12 Q Loss: 1.7092237612814642e-05\n",
      "Epoch: 1200 L Loss: 2.0480821899537816e-12 Q Loss: 1.29096533783013e-05\n",
      "Epoch: 1250 L Loss: 2.048018655706474e-12 Q Loss: 9.751320249051787e-06\n",
      "Epoch: 1300 L Loss: 2.0480852257198645e-12 Q Loss: 7.366213139903266e-06\n",
      "Epoch: 1350 L Loss: 2.0480438091968756e-12 Q Loss: 5.56481973035261e-06\n",
      "Epoch: 1400 L Loss: 2.048053567016428e-12 Q Loss: 4.20421065427945e-06\n",
      "Epoch: 1450 L Loss: 2.0480277630047228e-12 Q Loss: 3.1766521715326235e-06\n",
      "Epoch: 1500 L Loss: 2.0480342682177577e-12 Q Loss: 2.4006380954233464e-06\n",
      "Epoch: 1550 L Loss: 2.048091947773334e-12 Q Loss: 1.8142189901482197e-06\n",
      "Epoch: 1600 L Loss: 2.0480188725469084e-12 Q Loss: 1.3711376141145593e-06\n",
      "Epoch: 1650 L Loss: 2.048085442560299e-12 Q Loss: 1.0363680758018745e-06\n",
      "Epoch: 1700 L Loss: 2.048018655706474e-12 Q Loss: 7.83327834597003e-07\n",
      "Epoch: 1750 L Loss: 2.0480821899537816e-12 Q Loss: 5.921389742979954e-07\n",
      "Epoch: 1800 L Loss: 2.0480184388660394e-12 Q Loss: 4.47549666660052e-07\n",
      "Epoch: 1850 L Loss: 2.0480852257198645e-12 Q Loss: 3.3831685186669347e-07\n",
      "Epoch: 1900 L Loss: 2.0480438091968756e-12 Q Loss: 2.5574919959581166e-07\n",
      "Epoch: 1950 L Loss: 2.048053567016428e-12 Q Loss: 1.9332745182509825e-07\n",
      "Epoch: 2000 L Loss: 2.0480277630047228e-12 Q Loss: 1.461096417187946e-07\n",
      "Epoch: 2050 L Loss: 2.0480503144099105e-12 Q Loss: 1.1047281134324294e-07\n",
      "Epoch: 2100 L Loss: 2.0480761184216156e-12 Q Loss: 8.357973513284378e-08\n",
      "Epoch: 2150 L Loss: 2.0480188725469084e-12 Q Loss: 6.317322487348065e-08\n",
      "Epoch: 2200 L Loss: 2.048085442560299e-12 Q Loss: 4.7741298914161234e-08\n",
      "Epoch: 2250 L Loss: 2.048018655706474e-12 Q Loss: 3.61010492611058e-08\n",
      "Epoch: 2300 L Loss: 2.0480821899537816e-12 Q Loss: 2.7322732520929094e-08\n",
      "Epoch: 2350 L Loss: 2.0480184388660394e-12 Q Loss: 2.065604753909156e-08\n",
      "Epoch: 2400 L Loss: 2.0480852257198645e-12 Q Loss: 1.560494489183384e-08\n",
      "Epoch: 2450 L Loss: 2.0480438091968756e-12 Q Loss: 1.1776812414154847e-08\n",
      "Epoch: 2500 L Loss: 2.048053567016428e-12 Q Loss: 8.888702218712297e-09\n",
      "Epoch: 2550 L Loss: 2.0480277630047228e-12 Q Loss: 6.713886779863287e-09\n",
      "Epoch: 2600 L Loss: 2.0480503144099105e-12 Q Loss: 5.075415199229383e-09\n",
      "Epoch: 2650 L Loss: 2.0480761184216156e-12 Q Loss: 3.840323170578586e-09\n",
      "Epoch: 2700 L Loss: 2.0480188725469084e-12 Q Loss: 2.909397389672108e-09\n",
      "Epoch: 2750 L Loss: 2.048085442560299e-12 Q Loss: 2.2070369976034954e-09\n",
      "Epoch: 2800 L Loss: 2.048018655706474e-12 Q Loss: 1.676763616842436e-09\n",
      "Epoch: 2850 L Loss: 2.0480821899537816e-12 Q Loss: 1.2760338341522015e-09\n",
      "Epoch: 2900 L Loss: 2.0480184388660394e-12 Q Loss: 9.729836936145375e-10\n",
      "Epoch: 2950 L Loss: 2.0480852257198645e-12 Q Loss: 7.437819804501089e-10\n",
      "Epoch: 3000 L Loss: 2.0480438091968756e-12 Q Loss: 5.69916891457467e-10\n",
      "Epoch: 3050 L Loss: 2.048053567016428e-12 Q Loss: 4.3812825589739646e-10\n",
      "Epoch: 3100 L Loss: 2.0480277630047228e-12 Q Loss: 3.379162505368072e-10\n",
      "Epoch: 3150 L Loss: 2.0480503144099105e-12 Q Loss: 2.6155769217162117e-10\n",
      "Epoch: 3200 L Loss: 2.0480761184216156e-12 Q Loss: 2.0362869723733468e-10\n",
      "Epoch: 3250 L Loss: 2.0480188725469084e-12 Q Loss: 1.5913707851478165e-10\n",
      "Epoch: 3300 L Loss: 2.048085442560299e-12 Q Loss: 1.2511323921327033e-10\n",
      "Epoch: 3350 L Loss: 2.048018655706474e-12 Q Loss: 9.89131415662925e-11\n",
      "Epoch: 3400 L Loss: 2.0480821899537816e-12 Q Loss: 7.869452312014857e-11\n",
      "Epoch: 3450 L Loss: 2.0480184388660394e-12 Q Loss: 6.332787072516055e-11\n",
      "Epoch: 3500 L Loss: 2.0480852257198645e-12 Q Loss: 5.133446098759542e-11\n",
      "Epoch: 3550 L Loss: 2.0480438091968756e-12 Q Loss: 4.1734636579970896e-11\n",
      "Epoch: 3600 L Loss: 2.048053567016428e-12 Q Loss: 3.454088057242011e-11\n",
      "Epoch: 3650 L Loss: 2.048050531250345e-12 Q Loss: 2.900474305178591e-11\n",
      "Epoch: 3700 L Loss: 2.0480281966855918e-12 Q Loss: 2.4177413543435833e-11\n",
      "Epoch: 3750 L Loss: 2.0480761184216156e-12 Q Loss: 2.071709470641281e-11\n",
      "Epoch: 3800 L Loss: 2.0480188725469084e-12 Q Loss: 1.7993353335077344e-11\n",
      "Epoch: 3850 L Loss: 2.048085442560299e-12 Q Loss: 1.5814360238008973e-11\n",
      "Epoch: 3900 L Loss: 2.048018655706474e-12 Q Loss: 1.3643802233842539e-11\n",
      "Epoch: 3950 L Loss: 2.0480821899537816e-12 Q Loss: 1.1987250601863675e-11\n",
      "Epoch: 4000 L Loss: 2.0480184388660394e-12 Q Loss: 1.1178851247461985e-11\n",
      "Epoch: 4050 L Loss: 2.0480852257198645e-12 Q Loss: 1.0407625282427002e-11\n",
      "Epoch: 4100 L Loss: 2.0480438091968756e-12 Q Loss: 9.610792196801388e-12\n",
      "Epoch: 4150 L Loss: 2.048053567016428e-12 Q Loss: 8.696617211090274e-12\n",
      "Epoch: 4200 L Loss: 2.048050531250345e-12 Q Loss: 8.32021517654935e-12\n",
      "Epoch: 4250 L Loss: 2.0480281966855918e-12 Q Loss: 7.605318284864548e-12\n",
      "Epoch: 4300 L Loss: 2.0480761184216156e-12 Q Loss: 7.191585868482342e-12\n",
      "Epoch: 4350 L Loss: 2.0480188725469084e-12 Q Loss: 7.139288292490331e-12\n",
      "Epoch: 4400 L Loss: 2.048085442560299e-12 Q Loss: 6.9756865214709585e-12\n",
      "Epoch: 4450 L Loss: 2.048018655706474e-12 Q Loss: 6.612241136572106e-12\n",
      "Epoch: 4500 L Loss: 2.0480821899537816e-12 Q Loss: 6.596822914317624e-12\n",
      "Epoch: 4550 L Loss: 2.0480184388660394e-12 Q Loss: 6.578882837809941e-12\n",
      "Epoch: 4600 L Loss: 2.0480852257198645e-12 Q Loss: 6.291717147277609e-12\n",
      "Epoch: 4650 L Loss: 2.0480438091968756e-12 Q Loss: 5.779375675946108e-12\n",
      "Epoch: 4700 L Loss: 2.048053567016428e-12 Q Loss: 5.756248342564385e-12\n",
      "Epoch: 4750 L Loss: 2.048050531250345e-12 Q Loss: 5.734249446803785e-12\n",
      "Epoch: 4800 L Loss: 2.0480281966855918e-12 Q Loss: 5.726663934724208e-12\n",
      "Epoch: 4850 L Loss: 2.0480761184216156e-12 Q Loss: 5.727113228104486e-12\n",
      "Epoch: 4900 L Loss: 2.0480188725469084e-12 Q Loss: 5.727978421438129e-12\n",
      "Epoch: 4950 L Loss: 2.048085442560299e-12 Q Loss: 5.7301888928273925e-12\n",
      "L: [[[5.945316239319709e-09]], [0.979954719543457], [[-0.8783990144729614]], [-3.9998323917388916]]\n",
      "Q: [[[-4.711507244792301e-06, 1.98119831651411e-09]], [1.8293116092681885], [[-0.2064051777124405, -1.5926644802093506]], [-1.9255645275115967]]\n"
     ]
    }
   ],
   "source": [
    "# train_data = SimpleFunctionDataset(x=random_x(1), fn=fn) # 1000 training examples\n",
    "trainer.print_params()\n",
    "trainer.plot_every = 50\n",
    "trainer.train(5000, train_data)\n",
    "trainer.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "23dcdacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "aa675b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.clearLossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac08cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
